{"cells": [{"cell_type": "code", "execution_count": 15, "id": "fe198ba5-9cf2-4b45-a58c-284ae9cad6c2", "metadata": {}, "outputs": [], "source": "from google.cloud import storage,bigquery\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nimport datetime\nimport json"}, {"cell_type": "code", "execution_count": 16, "id": "1d868dc5-e82a-4ee1-a06d-81d223ad277d", "metadata": {}, "outputs": [], "source": "from typing import Optional,Union"}, {"cell_type": "code", "execution_count": 17, "id": "004fdacb-df3a-4176-bcfd-9ef1f97334df", "metadata": {}, "outputs": [], "source": "# initialize spark session\nspark = SparkSession.builder\\\n.appName(\"SupplierDataToGCSLanding\")\\\n.config(\"spark.jars.packages\",\"org.postgresql:postgresql:42.7.7\")\\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 18, "id": "f1a5ec86-ce27-4d60-b7f9-ccad3581321a", "metadata": {}, "outputs": [], "source": "# initialize gcs and bigquery clients\nstorage_client = storage.Client()\nbigquery_client = bigquery.Client()"}, {"cell_type": "code", "execution_count": 19, "id": "9ead352d-3082-4e37-99c6-6fb646357539", "metadata": {}, "outputs": [], "source": "# google cloud storage config variables\nGCS_BUCKET = \"retailer-datalake\"\n# path: bucketname/landing/supplier-db/table-name/table_name_DDMMYYYY.json\nLANDING_PATH = f\"gs://{GCS_BUCKET}/landing/supplier-db/\"\n# store previous day data in a heirarchical format\n# archive/YYYY/MM/DD/table-name/table_name_DDMMYYYY.json\nARCHIVE = f\"gs://{GCS_BUCKET}/landing/supplier-db/archive/\"\n# where metadata about our tables are stored\n# whether to load them in incr or full load pattern\nCONFIG_FILE_PATH = f\"gs://{GCS_BUCKET}/configs/supplier_config.csv\""}, {"cell_type": "code", "execution_count": 20, "id": "c0f41ffc-6d9d-4141-b51d-e82567026de4", "metadata": {}, "outputs": [], "source": "# bigquery configuration\nBIGQUERY_PROJECT = \"x-signifier-461105-s6\"\nBQ_AUDIT_TABLE=f\"{BIGQUERY_PROJECT}.temp_dataset.audit_log\"\nBQ_PIPELINE_LOGS=f\"{BIGQUERY_PROJECT}.temp_dataset.pipeline_logs\"\nBQ_TEMP_PATH=f\"{GCS_BUCKET}/temp/\""}, {"cell_type": "code", "execution_count": 21, "id": "8d92137d-b853-4b9e-9018-59d63349b1ef", "metadata": {}, "outputs": [], "source": "# PostgreSQL JDBC Configuration\nPOSTGRES_CONFIG = {\n    \"url\": \"jdbc:postgresql://34.131.224.75:5432/supplier_db\",\n    \"driver\": \"org.postgresql.Driver\",\n    \"user\": \"supplier_user\",\n    \"password\": \"pass123\"\n}"}, {"cell_type": "code", "execution_count": 22, "id": "a1152a0d-d3af-42b2-91fb-d8dee2696f16", "metadata": {}, "outputs": [], "source": "# save logs\n# logging mechanism\nlog_entries = [] # stores logs before writing to gcs\ndef log_event(event_type: str, message: str, table_name: Optional[str] = None):\n    \"\"\"\n    Logs an pipeline event to the log list.\n\n    Args:\n        event_type (str): The type/category of the event (\"INFO\", \"ERROR\", \"SUCCESS\").\n        msg (str): The log message to be recorded.\n        table_name (Optional[str], optional): The name of the related table, if applicable.\n    Returns:\n        None\n    \"\"\"\n    log_entry = {\n        \"timestamp\":datetime.datetime.now().isoformat(),\n        \"event_type\":event_type,\n        \"message\":message,\n        \"table\":table_name\n    }\n    log_entries.append(log_entry)\n    print(f\"[{log_entry['timestamp']}] {event_type} - {message}\")\n\n\ndef save_logs_to_gcs():\n    \"\"\"\n    Save the current pipeline logs to gcs\n    Returns:\n        None\n    \"\"\"\n    log_filename=f\"supplier_pipeline_log_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n    log_file_path=f\"temp/pipeline_logs/{log_filename}\"\n    json_data = json.dumps(log_entries,indent=4)\n    # gcs bucket\n    bucket = storage_client.bucket(GCS_BUCKET)\n    blob = bucket.blob(log_file_path)\n    # upload file to bucket\n    blob.upload_from_string(json_data,content_type=\"application/json\")\n    print(f\"logs successfully saved to GCS at gs://{GCS_BUCKET}/{log_file_path}\")\n    \n\ndef save_logs_to_bigquery():\n    \"\"\"\n    Save the current pipeline logs to bigquery\n    Returns:\n        None\n    \"\"\"\n    if log_entries:\n        log_df = spark.createDataFrame(log_entries)\n        log_df.write.format(\"bigquery\") \\\n            .option(\"table\", BQ_PIPELINE_LOGS) \\\n            .option(\"temporaryGcsBucket\", BQ_TEMP_PATH) \\\n            .mode(\"append\") \\\n            .save()\n        print(f\"logs stored in BigQuery table: {BQ_PIPELINE_LOGS}\")"}, {"cell_type": "code", "execution_count": 23, "id": "8626f4d2-2f25-4459-b873-d113c94fc8d3", "metadata": {}, "outputs": [], "source": "# get watermark\ndef get_latest_watermark(table_name:str):\n    \"\"\"\n    Get the latest watermark(updated_at field for a table) frpm the audit_table in bigquery.\n    Args:\n        table_name (str): name of the table\n    Returns:\n        Union[datetime.datetime, str]\n    \"\"\"\n    query = f\"\"\"\n    SELECT MAX(load_timestamp) as latest_timestamp\n    from `{BQ_AUDIT_TABLE}`\n    where tablename = '{table_name}'\n    \"\"\"\n    job = bigquery_client.query(query)\n    result = job.result()\n    for row in result:\n        return row.latest_timestamp if row.latest_timestamp else \"1900-01-01 00:00:00\"\n    return \"1900-01-01 00:00:00\""}, {"cell_type": "code", "execution_count": 24, "id": "9287597b-8955-4b70-b625-cdb1d3efabe2", "metadata": {}, "outputs": [], "source": "# move old data to archive\ndef move_existing_files_to_archive(table_name:str,target_path:str):\n    \"\"\"\n    Moves the existing(previous day) files to the archive.\n    archive structure:\n        archive\n        |______YYYY\n        |__________MM\n        |____________DD\n        |______________table_name_DDMMYYY.json\n    Args:\n        table_name (str): name of the table\n    Returns:\n        None\n    \"\"\"\n    # get the blobs from the gcs landing folder\n    bucket = storage_client.bucket(GCS_BUCKET)\n    blobs = bucket.list_blobs(prefix=f\"{target_path}/\")\n    existing_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n    if not existing_files:\n        log_event(\"INFO\",f\"No existing files for table {table_name}\")\n        return\n    for file in existing_files:\n        # extract the json file from the folder\n        source_blob = bucket.blob(file)\n        \n        # extract date from the file name\n        date_part = file.split(\"_\")[-1].split(\".\")[0] # ex: ../../suppliers_12062025.json -> 12062025\n        year,month,day = date_part[-4:],date_part[2:4],date_part[:2]\n        \n        # move the file in archive folder\n        archive_path = f\"landing/supplier-db/archive/{table_name}/{year}/{month}/{day}/{file.split('/')[-1]}\"\n        destination_blob = bucket.blob(archive_path)\n        \n        # copy the original blob from the source folder to the destination folder\n        bucket.copy_blob(source_blob,bucket,destination_blob.name)\n        source_blob.delete()\n        \n        log_event(\"INFO\",f\"Moved {file} to {archive_path}\",table_name=table_name)"}, {"cell_type": "code", "execution_count": 25, "id": "f5b8d525-dbfe-4b7c-8a10-e0e4936b3721", "metadata": {}, "outputs": [], "source": "# save to gcs datalake\ndef extract_and_save_to_landing_gcs(table_name:str,load_type:str,watermark_col:str,target_path:str):\n    \"\"\"\n    Loads the data for the current date in json format to gcs landing folder\n    Args:\n        table_name (str): name of the table\n        load_type (str): whether the table is incremental or full load type\n        watermark_col: field which defines the latest load time of the data\n    Returns:\n        None\n    \"\"\"\n    log_event(\"INFO\", f\"Starting data extraction for table: {table_name} (Load Type: {load_type})\", table_name=table_name)\n    try:\n        # get latest watermark\n        last_watermark = None\n        query = None\n        if load_type.lower() == \"incremental\":\n            last_watermark = get_latest_watermark(table_name=table_name)\n            log_event(\"INFO\",f\"Last watermark for {table_name}:{last_watermark}\",table_name=table_name)\n            # query based on load type = incremental\n            query = f\"(SELECT * FROM {table_name} WHERE {watermark_col} > '{last_watermark}') as t\"\n        else:\n            # query based on load type = full load\n            query = f\"(SELECT * FROM {table_name}) as t\"\n        # read data from cloud sql(postgresql) table\n        print(query)\n        table_df = (\n        spark.read.format(\"jdbc\")\n            .option(\"url\", POSTGRES_CONFIG[\"url\"]) \\\n            .option(\"dbtable\", query) \\\n            .option(\"user\", POSTGRES_CONFIG[\"user\"]) \\\n            .option(\"password\", POSTGRES_CONFIG[\"password\"]) \\\n            .option(\"driver\", POSTGRES_CONFIG[\"driver\"]) \\\n            .load()\n        )\n        log_event(\"SUCCESS\", f\"successfully extracted data from {table_name}\", table_name=table_name)\n        # convert spark df -> json type\n        pandas_dataframe = table_df.toPandas()\n        json_data = pandas_dataframe.to_json(orient=\"records\",lines=True)\n        \n        # json file path in gcs\n        today = datetime.datetime.today().strftime(\"%d%m%Y\")\n        JSON_FILE_PATH = f\"{target_path}/{table_name}_{today}.json\"\n        \n        # upload json data to gcs\n        bucket = storage_client.bucket(GCS_BUCKET)\n        blob = bucket.blob(JSON_FILE_PATH)\n        blob.upload_from_string(json_data,content_type=\"application/json\")\n        log_event(\"SUCCESS\", f\"JSON file successfully written to gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table_name=table_name)\n        \n        # add entry in audit table\n        # add current ingestion timestamp for current table\n        audit_df = (\n            spark.createDataFrame([(table_name,load_type.lower(),table_df.count(),datetime.datetime.now(),\"SUCCESS\")],\n                                  [\"tablename\",\"load_type\",\"record_count\",\"load_timestamp\",\"status\"]\n                                 )\n        )\n        (\n        audit_df.write.format(\"bigquery\")\n            .option(\"table\",BQ_AUDIT_TABLE)\n            .option(\"temporaryGcsBucket\",GCS_BUCKET)\n            .mode(\"append\")\n            .save()\n        )\n        log_event(\"SUCCESS\", f\"audit log updated for {table_name} at {datetime.datetime.now()}\", table_name=table_name)\n    except Exception as e:\n        log_event(\"ERROR\", f\"fn_extract_and_save_to_landing_gcs error processing {table_name}: {str(e)}\", table_name=table_name)"}, {"cell_type": "code", "execution_count": 26, "id": "0d793c81-01af-4eb5-920a-d7cae1fc7b5f", "metadata": {}, "outputs": [], "source": "def read_config_file(file_path:str):\n    \"\"\"\n    Read config file for metadata about the current table\n    \n    Args:\n        file_path (str): Location of the file path in gcs bucket\n    Returns:\n        Spark DataFrame\n    \"\"\"\n    try:\n        df = (\n            spark.read.format(\"csv\")\n            .option(\"header\", \"true\")\n            .option(\"inferSchema\", \"true\")\n            .load(file_path)\n        )\n        log_event(\"INFO\",\"successfully read the config file\")\n        return df\n    except Exception as e:\n        log_event(\"ERROR\", f\"Error reading file: {str(e)}\")\n        return None"}, {"cell_type": "code", "execution_count": 27, "id": "feb626ba-cc82-4f34-917d-ae8b2c0e6abe", "metadata": {}, "outputs": [], "source": "# main_process execution\ndef main_process():\n    log_event(\"INFO\", \"Started data ingestion process\")\n    config_df = read_config_file(CONFIG_FILE_PATH)\n    for row in config_df.collect():\n        if row[\"is_active\"] == 1:\n            db,db_src,table,load_type,watermark,is_active,target_path = row\n            move_existing_files_to_archive(table_name=table,target_path=target_path) # shift the existing file to archive folder\n            extract_and_save_to_landing_gcs(table_name=table,load_type=load_type,watermark_col=watermark,target_path=target_path)\n    # save logs in gcs and bigquery\n    save_logs_to_gcs()\n    save_logs_to_bigquery()\n    print(\"Done\")"}, {"cell_type": "code", "execution_count": 28, "id": "bf352c26-bb6e-41d3-936f-7055cfedb3cb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:46:45.939176] INFO - Started data ingestion process\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:46:49.970785] INFO - successfully read the config file\n[2025-07-19T11:46:50.405308] INFO - No existing files for table suppliers\n[2025-07-19T11:46:50.405398] INFO - Starting data extraction for table: suppliers (Load Type: Full Load)\n(SELECT * FROM suppliers) as t\n[2025-07-19T11:46:50.463068] SUCCESS - successfully extracted data from suppliers\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:46:51.533187] SUCCESS - JSON file successfully written to gs://retailer-datalake/landing/supplier-db/supplier/suppliers_19072025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:46:56.547083] SUCCESS - audit log updated for suppliers at 2025-07-19 11:46:56.547050\n[2025-07-19T11:46:56.566241] INFO - No existing files for table product_suppliers\n[2025-07-19T11:46:56.566281] INFO - Starting data extraction for table: product_suppliers (Load Type: Incremental)\n[2025-07-19T11:46:57.386580] INFO - Last watermark for product_suppliers:1900-01-01 00:00:00\n(SELECT * FROM product_suppliers WHERE last_updated > '1900-01-01 00:00:00') as t\n[2025-07-19T11:46:57.426386] SUCCESS - successfully extracted data from product_suppliers\n[2025-07-19T11:46:57.651299] SUCCESS - JSON file successfully written to gs://retailer-datalake/landing/supplier-db/product_suppliers/product_suppliers_19072025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:47:02.483224] SUCCESS - audit log updated for product_suppliers at 2025-07-19 11:47:02.483190\nlogs successfully saved to GCS at gs://retailer-datalake/temp/pipeline_logs/supplier_pipeline_log_20250719114702.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "logs stored in BigQuery table: x-signifier-461105-s6.temp_dataset.pipeline_logs\nDone\n"}], "source": "main_process()"}, {"cell_type": "code", "execution_count": null, "id": "c95d9f7d-bdf0-4347-a742-b5f3e18ae5c8", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}