{"cells": [{"cell_type": "code", "execution_count": 17, "id": "380976e9-6b13-46c4-9841-0b2e5a002884", "metadata": {}, "outputs": [], "source": "from google.cloud import storage,bigquery\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nimport datetime\nimport json"}, {"cell_type": "code", "execution_count": 18, "id": "24ae8b48-d24f-4444-92f3-74da38f96bea", "metadata": {}, "outputs": [], "source": "from typing import Optional,Union"}, {"cell_type": "code", "execution_count": 19, "id": "3825c930-af4a-4cdc-bf8b-95f059596f98", "metadata": {}, "outputs": [], "source": "# initialize spark session\nspark = SparkSession.builder\\\n.appName(\"RetailerDataToGCSLanding\")\\\n.config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.7\")\\\n.config(\"spark.sql.adaptive.enabled\", \"true\")\\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 20, "id": "ed3f7832-e550-40a8-a026-3a2d1396f628", "metadata": {}, "outputs": [], "source": "# google cloud storage config variables\nGCS_BUCKET = \"retailer-datalake\"\n# path: bucketname/landing/retailer-db/table-name/table_name_DDMMYYYY.json\nLANDING_PATH = f\"gs://{GCS_BUCKET}/landing/retailer-db/\"\n# store previous day data in a heirarchical format\n# archive/YYYY/MM/DD/table-name/table_name_DDMMYYYY.json\nARCHIVE = f\"gs://{GCS_BUCKET}/landing/retailer-db/archive/\"\n# where metadata about our tables are stored\n# whether to load them in incr or full load pattern\nCONFIG_FILE_PATH = f\"gs://{GCS_BUCKET}/configs/retail_config.csv\""}, {"cell_type": "code", "execution_count": 21, "id": "67025729-aa23-40e9-ba6a-fcc5cca65826", "metadata": {}, "outputs": [], "source": "# bigquery configuration\nBIGQUERY_PROJECT = \"x-signifier-461105-s6\"\nBQ_AUDIT_TABLE=f\"{BIGQUERY_PROJECT}.temp_dataset.audit_log\"\nBQ_PIPELINE_LOGS=f\"{BIGQUERY_PROJECT}.temp_dataset.pipeline_logs\"\nBQ_TEMP_PATH=f\"{GCS_BUCKET}/temp/\""}, {"cell_type": "code", "execution_count": 22, "id": "9ee90f52-2b3e-42f5-808f-01c625818764", "metadata": {}, "outputs": [], "source": "# PostgreSQL JDBC Configuration\nPOSTGRES_CONFIG = {\n    \"url\": \"jdbc:postgresql://34.131.208.15:5432/retailer_db\",\n    \"driver\": \"org.postgresql.Driver\",\n    \"user\": \"retailer_user\",\n    \"password\": \"pass123\"\n}"}, {"cell_type": "code", "execution_count": 23, "id": "22030436-cdfb-48a8-ae2b-73813c2d618f", "metadata": {}, "outputs": [], "source": "# initialize gcs and bigquery clients\nstorage_client = storage.Client()\nbigquery_client = bigquery.Client()"}, {"cell_type": "code", "execution_count": 24, "id": "241b01a3-10c8-4f62-b607-baa405204c08", "metadata": {}, "outputs": [], "source": "# logging mechanism\nlog_entries = [] # stores logs before writing to gcs\ndef log_event(event_type: str, message: str, table_name: Optional[str] = None):\n    \"\"\"\n    Logs an pipeline event to the log list.\n\n    Args:\n        event_type (str): The type/category of the event (\"INFO\", \"ERROR\", \"SUCCESS\").\n        msg (str): The log message to be recorded.\n        table_name (Optional[str], optional): The name of the related table, if applicable.\n    Returns:\n        None\n    \"\"\"\n    log_entry = {\n        \"timestamp\":datetime.datetime.now().isoformat(),\n        \"event_type\":event_type,\n        \"message\":message,\n        \"table\":table_name\n    }\n    log_entries.append(log_entry)\n    print(f\"[{log_entry['timestamp']}] {event_type} - {message}\")"}, {"cell_type": "code", "execution_count": 25, "id": "975b8166-677e-4e39-9939-a624a8de18b2", "metadata": {}, "outputs": [], "source": "def read_config_file(file_path:str):\n    \"\"\"\n    Read config file for metadata about the current table\n    \n    Args:\n        file_path (str): Location of the file path in gcs bucket\n    Returns:\n        Spark DataFrame\n    \"\"\"\n    try:\n        df = (\n            spark.read.format(\"csv\")\n            .option(\"header\", \"true\")\n            .option(\"inferSchema\", \"true\")\n            .load(file_path)\n        )\n        log_event(\"INFO\",\"successfully read the config file\")\n        return df\n    except Exception as e:\n        log_event(\"ERROR\", f\"Error reading file: {str(e)}\")\n        return None"}, {"cell_type": "code", "execution_count": 26, "id": "7f48fd41-1fa9-4d16-8b2a-17aabb86db88", "metadata": {}, "outputs": [], "source": "def move_existing_files_to_archive(table_name:str,target_path:str):\n    \"\"\"\n    Moves the existing(previous day) files to the archive.\n    archive structure:\n        archive\n        |______YYYY\n        |__________MM\n        |____________DD\n        |______________table_name_DDMMYYY.json\n    Args:\n        table_name (str): name of the table\n    Returns:\n        None\n    \"\"\"\n    # get the blobs from the gcs landing folder\n    bucket = storage_client.bucket(GCS_BUCKET)\n    blobs = bucket.list_blobs(prefix=f\"{target_path}/\")\n    existing_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n    if not existing_files:\n        log_event(\"INFO\",f\"No existing files for table {table_name}\")\n        return\n    for file in existing_files:\n        # extract the json file from the folder\n        source_blob = bucket.blob(file)\n        \n        # extract date from the file name\n        date_part = file.split(\"_\")[-1].split(\".\")[0] # ex: ../../products_12062025.json -> 12062025\n        year,month,day = date_part[-4:],date_part[2:4],date_part[:2]\n        \n        # move the file in archive folder\n        archive_path = f\"landing/retailer-db/archive/{table_name}/{year}/{month}/{day}/{file.split('/')[-1]}\"\n        destination_blob = bucket.blob(archive_path)\n        \n        # copy the original blob from the source folder to the destination folder\n        bucket.copy_blob(source_blob,bucket,destination_blob.name)\n        source_blob.delete()\n        \n        log_event(\"INFO\",f\"Moved {file} to {archive_path}\",table_name=table_name)"}, {"cell_type": "code", "execution_count": 27, "id": "5dc91501-c21a-4c2a-a402-fb82fd7040f3", "metadata": {}, "outputs": [], "source": "def get_latest_watermark(table_name:str):\n    \"\"\"\n    Get the latest watermark(updated_at field for a table) frpm the audit_table in bigquery.\n    Args:\n        table_name (str): name of the table\n    Returns:\n        Union[datetime.datetime, str]\n    \"\"\"\n    query = f\"\"\"\n    SELECT MAX(load_timestamp) as latest_timestamp\n    from `{BQ_AUDIT_TABLE}`\n    where tablename = '{table_name}'\n    \"\"\"\n    job = bigquery_client.query(query)\n    result = job.result()\n    for row in result:\n        return row.latest_timestamp if row.latest_timestamp else \"1900-01-01 00:00:00\"\n    return \"1900-01-01 00:00:00\""}, {"cell_type": "code", "execution_count": 28, "id": "6efabf15-9f27-41ea-9d53-830c08fcac96", "metadata": {}, "outputs": [], "source": "def extract_and_save_to_landing_gcs(table_name:str,load_type:str,watermark_col:str,target_path:str):\n    \"\"\"\n    Loads the data for the current date in json format to gcs landing folder\n    Args:\n        table_name (str): name of the table\n        load_type (str): whether the table is incremental or full load type\n        watermark_col: field which defines the latest load time of the data\n    Returns:\n        None\n    \"\"\"\n    log_event(\"INFO\", f\"Starting data extraction for table: {table_name} (Load Type: {load_type})\", table_name=table_name)\n    try:\n        # get latest watermark\n        last_watermark = None\n        query = None\n        if load_type.lower() == \"incremental\":\n            last_watermark = get_latest_watermark(table_name=table_name)\n            log_event(\"INFO\",f\"Last watermark for {table_name}:{last_watermark}\",table_name=table_name)\n            # query based on load type = incremental\n            query = f\"(SELECT * FROM {table_name} WHERE {watermark_col} > '{last_watermark}') as t\"\n        else:\n            # query based on load type = full load\n            query = f\"(SELECT * FROM {table_name}) as t\"\n        # read data from cloud sql(postgresql) table\n        print(query)\n        table_df = (\n        spark.read.format(\"jdbc\")\n            .option(\"url\", POSTGRES_CONFIG[\"url\"]) \\\n            .option(\"dbtable\", query) \\\n            .option(\"user\", POSTGRES_CONFIG[\"user\"]) \\\n            .option(\"password\", POSTGRES_CONFIG[\"password\"]) \\\n            .option(\"driver\", POSTGRES_CONFIG[\"driver\"]) \\\n            .load()\n        )\n        log_event(\"SUCCESS\", f\"successfully extracted data from {table_name}\", table_name=table_name)\n        # convert spark df -> json type\n        pandas_dataframe = table_df.toPandas()\n        json_data = pandas_dataframe.to_json(orient=\"records\",lines=True)\n        \n        # json file path in gcs\n        today = datetime.datetime.today().strftime(\"%d%m%Y\")\n        JSON_FILE_PATH = f\"{target_path}/{table_name}_{today}.json\"\n        \n        # upload json data to gcs\n        bucket = storage_client.bucket(GCS_BUCKET)\n        blob = bucket.blob(JSON_FILE_PATH)\n        blob.upload_from_string(json_data,content_type=\"application/json\")\n        log_event(\"SUCCESS\", f\"JSON file successfully written to gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table_name=table_name)\n        \n        # add entry in audit table\n        # add current ingestion timestamp for current table\n        audit_df = (\n            spark.createDataFrame([(table_name,load_type.lower(),table_df.count(),datetime.datetime.now(),\"SUCCESS\")],\n                                  [\"tablename\",\"load_type\",\"record_count\",\"load_timestamp\",\"status\"]\n                                 )\n        )\n        (\n        audit_df.write.format(\"bigquery\")\n            .option(\"table\",BQ_AUDIT_TABLE)\n            .option(\"temporaryGcsBucket\",GCS_BUCKET)\n            .mode(\"append\")\n            .save()\n        )\n        log_event(\"SUCCESS\", f\"audit log updated for {table_name} at {datetime.datetime.now()}\", table_name=table_name)\n    except Exception as e:\n        log_event(\"ERROR\", f\"fn_extract_and_save_to_landing_gcs error processing {table_name}: {str(e)}\", table_name=table_name)"}, {"cell_type": "code", "execution_count": 29, "id": "afd624f1-5fbf-46a8-a1ec-df21c50d9084", "metadata": {}, "outputs": [], "source": "def save_logs_to_gcs():\n    \"\"\"\n    Save the current pipeline logs to gcs\n    Returns:\n        None\n    \"\"\"\n    log_filename=f\"retail_pipeline_log_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n    log_file_path=f\"temp/pipeline_logs/{log_filename}\"\n    json_data = json.dumps(log_entries,indent=4)\n    # gcs bucket\n    bucket = storage_client.bucket(GCS_BUCKET)\n    blob = bucket.blob(log_file_path)\n    # upload file to bucket\n    blob.upload_from_string(json_data,content_type=\"application/json\")\n    print(f\"logs successfully saved to GCS at gs://{GCS_BUCKET}/{log_file_path}\")"}, {"cell_type": "code", "execution_count": 30, "id": "827ee74f-b1d4-47dd-9be3-1e421a05e440", "metadata": {}, "outputs": [], "source": "def save_logs_to_bigquery():\n    \"\"\"\n    Save the current pipeline logs to bigquery\n    Returns:\n        None\n    \"\"\"\n    if log_entries:\n        log_df = spark.createDataFrame(log_entries)\n        log_df.write.format(\"bigquery\") \\\n            .option(\"table\", BQ_PIPELINE_LOGS) \\\n            .option(\"temporaryGcsBucket\", BQ_TEMP_PATH) \\\n            .mode(\"append\") \\\n            .save()\n        print(f\"logs stored in BigQuery table: {BQ_PIPELINE_LOGS}\")"}, {"cell_type": "code", "execution_count": 31, "id": "b79058ae-a979-4696-a917-df6d1ad28c87", "metadata": {}, "outputs": [], "source": "# main_process execution\ndef main_process():\n    log_event(\"INFO\", \"Started data ingestion process\")\n    config_df = read_config_file(CONFIG_FILE_PATH)\n    for row in config_df.collect():\n        if row[\"is_active\"] == 1:\n            db,db_src,table,load_type,watermark,is_active,target_path = row\n            move_existing_files_to_archive(table_name=table,target_path=target_path) # shift the existing file to archive folder\n            extract_and_save_to_landing_gcs(table_name=table,load_type=load_type,watermark_col=watermark,target_path=target_path)\n    # save logs in gcs and bigquery\n    save_logs_to_gcs()\n    save_logs_to_bigquery()\n    print(\"Done\")"}, {"cell_type": "code", "execution_count": null, "id": "26c85240-9c2c-4514-8e38-948890bb270f", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 32, "id": "627e8684-e8c5-4c56-93cc-0b0f00012c55", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:39:43.661149] INFO - Started data ingestion process\n[2025-07-19T11:39:44.434526] INFO - successfully read the config file\n[2025-07-19T11:39:44.640439] INFO - No existing files for table products\n[2025-07-19T11:39:44.640519] INFO - Starting data extraction for table: products (Load Type: Full Load)\n(SELECT * FROM products) as t\n[2025-07-19T11:39:44.681763] SUCCESS - successfully extracted data from products\n[2025-07-19T11:39:44.832449] SUCCESS - JSON file successfully written to gs://retailer-datalake/landing/retailer-db/products/products_19072025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:39:50.272188] SUCCESS - audit log updated for products at 2025-07-19 11:39:50.272154\n[2025-07-19T11:39:50.285850] INFO - No existing files for table categories\n[2025-07-19T11:39:50.285890] INFO - Starting data extraction for table: categories (Load Type: Full Load)\n(SELECT * FROM categories) as t\n[2025-07-19T11:39:50.325130] SUCCESS - successfully extracted data from categories\n[2025-07-19T11:39:50.462883] SUCCESS - JSON file successfully written to gs://retailer-datalake/landing/retailer-db/categories/categories_19072025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:39:54.261499] SUCCESS - audit log updated for categories at 2025-07-19 11:39:54.261465\n[2025-07-19T11:39:54.279601] INFO - No existing files for table customers\n[2025-07-19T11:39:54.279643] INFO - Starting data extraction for table: customers (Load Type: Incremental)\n[2025-07-19T11:39:54.909190] INFO - Last watermark for customers:1900-01-01 00:00:00\n(SELECT * FROM customers WHERE updated_at > '1900-01-01 00:00:00') as t\n[2025-07-19T11:39:54.958022] SUCCESS - successfully extracted data from customers\n[2025-07-19T11:39:55.099687] SUCCESS - JSON file successfully written to gs://retailer-datalake/landing/retailer-db/customers/customers_19072025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:39:58.862695] SUCCESS - audit log updated for customers at 2025-07-19 11:39:58.862658\n[2025-07-19T11:39:58.874545] INFO - No existing files for table orders\n[2025-07-19T11:39:58.874588] INFO - Starting data extraction for table: orders (Load Type: Incremental)\n[2025-07-19T11:39:59.381771] INFO - Last watermark for orders:1900-01-01 00:00:00\n(SELECT * FROM orders WHERE updated_at > '1900-01-01 00:00:00') as t\n[2025-07-19T11:39:59.424325] SUCCESS - successfully extracted data from orders\n[2025-07-19T11:39:59.562962] SUCCESS - JSON file successfully written to gs://retailer-datalake/landing/retailer-db/orders/orders_19072025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:40:02.986844] SUCCESS - audit log updated for orders at 2025-07-19 11:40:02.986811\n[2025-07-19T11:40:03.003725] INFO - No existing files for table order_items\n[2025-07-19T11:40:03.003763] INFO - Starting data extraction for table: order_items (Load Type: Incremental)\n[2025-07-19T11:40:03.504485] INFO - Last watermark for order_items:1900-01-01 00:00:00\n(SELECT * FROM order_items WHERE updated_at > '1900-01-01 00:00:00') as t\n[2025-07-19T11:40:03.544052] SUCCESS - successfully extracted data from order_items\n[2025-07-19T11:40:03.668123] SUCCESS - JSON file successfully written to gs://retailer-datalake/landing/retailer-db/order_items/order_items_19072025.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-07-19T11:40:08.883591] SUCCESS - audit log updated for order_items at 2025-07-19 11:40:08.883563\nlogs successfully saved to GCS at gs://retailer-datalake/temp/pipeline_logs/retail_pipeline_log_20250719114008.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "logs stored in BigQuery table: x-signifier-461105-s6.temp_dataset.pipeline_logs\nDone\n"}], "source": "main_process()"}, {"cell_type": "code", "execution_count": null, "id": "8c5bce55-7fd7-498c-8209-8c2571ee068f", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}